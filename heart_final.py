# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wgwOIOF8byC_tG3mkl0xau1wLrV49_s-
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/heart.csv')

df.info()

df.shape

df.head()

df.isnull().sum()

df.columns

df.duplicated().sum()

def plotting(var,num):
    plt.subplot(2,2,num)
    sns.histplot(df[var],kde = True)

plotting('Age',1)
plotting('RestingBP',2)
plotting('Cholesterol',3)
plotting('MaxHR',4)


plt.tight_layout()

ch_mean = df.loc[df['Cholesterol'] != 0,'Cholesterol'].mean()

df['Cholesterol'] = df['Cholesterol'].replace(0,ch_mean)
df['Cholesterol'] = df['Cholesterol'].round(2)

resting_bp_mean = df.loc[df['RestingBP'] != 0, 'RestingBP'].mean()

df['RestingBP'] = df['RestingBP'].replace(0, resting_bp_mean)

df['RestingBP'] = df['RestingBP'].round(2)

df_encoded = pd.get_dummies(df,drop_first=True)

df_encoded

df_encoded = df_encoded.astype(int)

df_encoded

from sklearn.preprocessing import StandardScaler
numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
scaler = StandardScaler()
df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])
df_encoded.head()

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

X = df_encoded.drop('HeartDisease', axis=1)
y = df_encoded['HeartDisease']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(),
    "SVM (RBF Kernel)": SVC(probability=True)
}

results = []

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    results.append({
        'Model': name,
        'Accuracy': round(acc, 4),
        'F1 Score': round(f1, 4)
    })

results

from sklearn.ensemble import RandomForestClassifier

rf_model=RandomForestClassifier( n_estimators = 100,max_depth=None, random_state=42)

rf_model.fit(X_train_scaled,y_train)

y_pred=rf_model.predict(X_test_scaled)

accuracy_score(y_test,y_pred)

from xgboost import XGBClassifier

xgb_model=XGBClassifier(n_estimator=100,learning_rate=0.1,max_depth=3,use_label_encoder=False,eval_metric='logloss',random_state=42)

xgb_model.fit(X_train_scaled,y_train)

y_pred=xgb_model.predict(X_test_scaled)

accuracy=accuracy_score(y_test,y_pred)

accuracy

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 1, 5],
    'reg_lambda': [1, 1.5, 2]
}

grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,                # 5-fold cross-validation
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train_scaled, y_train)

print("Best Parameters:", grid_search.best_params_)

best_model = grid_search.best_estimator_

f_pred = best_model.predict(X_test_scaled)

acc=accuracy_score(y_test,f_pred)

acc

from sklearn.ensemble import StackingClassifier

base_learners = [
    ('xgb', XGBClassifier(n_estimator=100,learning_rate=0.1,max_depth=3,use_label_encoder=False,eval_metric='logloss',random_state=42)),
    ('rf', RandomForestClassifier( n_estimators = 100,max_depth=None, random_state=42)),
    ('lr', LogisticRegression(max_iter=1000))
]

meta_learner = LogisticRegression(max_iter=1000)

stacking_model = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5
)

stacking_model.fit(X_train_scaled, y_train)

s_pred = stacking_model.predict(X_test_scaled)


accu = accuracy_score(y_test, s_pred)

accu

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'gamma': [0, 1, 5]
}

xgb_search = RandomizedSearchCV(xgb, xgb_params, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
xgb_search.fit(X_train_scaled, y_train)
best_xgb = xgb_search.best_estimator_
print("Best XGB:", xgb_search.best_params_)

rf = RandomForestClassifier(random_state=42)
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf_search = RandomizedSearchCV(rf, rf_params, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
rf_search.fit(X_train_scaled, y_train)
best_rf = rf_search.best_estimator_
print("Best RF:", rf_search.best_params_)

lr = LogisticRegression(max_iter=500, solver='liblinear')
lr_params = {
    'C': np.logspace(-3, 3, 7),
    'penalty': ['l1', 'l2']
}

lr_search = RandomizedSearchCV(lr, lr_params, n_iter=5, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
lr_search.fit(X_train_scaled, y_train)
best_lr = lr_search.best_estimator_
print("Best LR:", lr_search.best_params_)

stack = StackingClassifier(
    estimators=[('xgb', best_xgb), ('rf', best_rf)],
    final_estimator=best_lr,
    n_jobs=-1
)

stack.fit(X_train_scaled, y_train)

y_pred = stack.predict(X_test_scaled)
print("Stacking Accuracy:", accuracy_score(y_test, y_pred))

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('logreg', LogisticRegression(max_iter=500))
])

param_grid = {
    'logreg__C': [0.01, 0.1, 1, 10, 100],         # Regularization strength
    'logreg__penalty': ['l1', 'l2', 'elasticnet'], # Penalty type
    'logreg__solver': ['saga'],                   # saga supports l1, l2, elasticnet
    'logreg__l1_ratio': [0, 0.5, 1]               # Only for elasticnet
}

grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
grid.fit(X_train_scaled, y_train)

print("Best Parameters:", grid.best_params_)

stack = StackingClassifier(
    estimators=[('xgb', best_xgb), ('rf', best_rf), ('lr_tuned', grid.best_estimator_)],
    final_estimator=best_lr,
    n_jobs=-1
)
be_lr = lr_search.best_estimator_

stack.fit(X_train_scaled, y_train)

b_pred = stack.predict(X_test_scaled)
print("Stacking Accuracy:", accuracy_score(y_test, b_pred))

import joblib
joblib.dump(stack,'stack_heart.pkl')
joblib.dump(scaler,'scaler.pkl')
joblib.dump(X.columns.tolist(),'columns.pkl')